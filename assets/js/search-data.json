{
  
    
        "post0": {
            "title": "Multilingual CLIP with Huggingface + PyTorch Lightning 🤗 ⚡",
            "content": "This is a walkthrough of training CLIP by OpenAI. CLIP was designed to put both images and text into a new projected space such that they can map to each other by simply looking at dot products. . Traditionally training sets like imagenet only allowed you to map images to a single class (and hence one word). This method allows you to map text to images, but can also be used to map images to text if the need arises. . This particular blog however is specifically how we managed to train this on colab GPUs using huggingface transformers and pytorch lightning. . Thanks to fastpages by fastai you can run this blog on colab using GPUS. . Acknowledgement . Kudos to the following CLIP tutorial in the keras documentation. . The important thing to notice about the constants is the embedding dim. We will project the output of a resnet and transformers into 512 dimensional space. . EMBED_DIM = 512 TRANSFORMER_EMBED_DIM = 768 MAX_LEN = 128 # Maximum length of text TEXT_MODEL = &quot;distilbert-base-multilingual-cased&quot; EPOCHS = 5 BATCH_SIZE = 64 . Data . We download the coco dataset which contains 5 captions per image and has roughly 82k images. We take 20% of it to be our validation set. . Considering that the image backbone is trained using imagenet, we normalise it using the imagenet stats as shown in the transforms normalize step. We also resize the image to 128x128 to make sure it trains in reasonable time. . Warning: Downloading the files will take a while (~5-10 minutes). . img = inv_tfm(img) plt.imshow(np.rot90(img.transpose(0, 2), 3)) plt.title(target) plt.show() . train_len = int(0.8*len(cap)) train_data, valid_data = random_split(cap, [train_len, len(cap) - train_len]) train_dl = DataLoader(train_data, BATCH_SIZE, pin_memory=True, shuffle=True, num_workers=4, drop_last=True) valid_dl = DataLoader(valid_data, BATCH_SIZE, pin_memory=True, shuffle=False, num_workers=4, drop_last=False) . Model . There are two main models, the VisionEncoder and the TextEncoder which have resnet18 and distilbert as backbones. In order to make it multi-lingual, we simply choose the distilbert-multilingual model and that&#39;s it! No need to specifically train on non-english words as you will soon see. . The Projection module, takes the embeddings from vision and text encoders and projects them into 512 dimensional space. . Two things to note: . We have frozen both the text and vision encoder backbones and do not retrain their weights at all. | For both encoders the final output is normalised to be of unit length. | class Projection(nn.Module): def __init__(self, d_in: int, d_out: int, p: float=0.5) -&gt; None: super().__init__() self.linear1 = nn.Linear(d_in, d_out, bias=False) self.linear2 = nn.Linear(d_out, d_out, bias=False) self.layer_norm = nn.LayerNorm(d_out) self.drop = nn.Dropout(p) def forward(self, x: torch.Tensor) -&gt; torch.Tensor: embed1 = self.linear1(x) embed2 = self.drop(self.linear2(F.gelu(embed1))) embeds = self.layer_norm(embed1 + embed2) return embeds . class VisionEncoder(nn.Module): def __init__(self, d_out: int) -&gt; None: super().__init__() base = models.resnet34(pretrained=True) d_in = base.fc.in_features base.fc = nn.Identity() self.base = base self.projection = Projection(d_in, d_out) for p in self.base.parameters(): p.requires_grad = False def forward(self, x): projected_vec = self.projection(self.base(x)) projection_len = torch.norm(projected_vec, dim=-1, keepdim=True) return projected_vec / projection_len . class TextEncoder(nn.Module): def __init__(self, d_out: int) -&gt; None: super().__init__() self.base = AutoModel.from_pretrained(TEXT_MODEL) self.projection = Projection(TRANSFORMER_EMBED_DIM, d_out) for p in self.base.parameters(): p.requires_grad = False def forward(self, x): out = self.base(**x)[0] out = out[:, 0, :] # get CLS token output projected_vec = self.projection(out) projection_len = torch.norm(projected_vec, dim=-1, keepdim=True) return projected_vec / projection_len class Tokenizer: def __init__(self, tokenizer: BertTokenizer) -&gt; None: self.tokenizer = tokenizer def __call__(self, x: str) -&gt; AutoTokenizer: return self.tokenizer( x, max_length=MAX_LEN, truncation=True, padding=True, return_tensors=&quot;pt&quot; ) . CLIP loss function . For someone like me who hasn&#39;t played around with contrastive loss, this was the most interesting part. . We know that we want the vectors of the corresponding image and the text to line up. Which means that the dot product has to be as close to one as possible. For everything else we need to push it towards 0. . Therfore for a given caption, we take the softmax of the dot products across all images, and then take cross entropy loss. Similarly for a given image, we repeat the process across all captions. We average these two losses. . In terms of which element is the true positive within a batch, remember that we are sending image, caption pairs already lined up. Therefore we want all the diagonal elements to line up while all off-diagonal elements we want to push towards zero. . def contrastive_loss(logits, dim): neg_ce = torch.diag(F.log_softmax(logits, dim=dim)) return -neg_ce.mean() def clip_loss(similarity: torch.Tensor) -&gt; torch.Tensor: caption_loss = contrastive_loss(similarity, dim=0) image_loss = contrastive_loss(similarity, dim=1) return (caption_loss + image_loss) / 2.0 def metrics(similarity: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]: y = torch.arange(len(similarity)).to(similarity.device) img2cap_match_idx = similarity.argmax(dim=1) cap2img_match_idx = similarity.argmax(dim=0) img_acc = (img2cap_match_idx == y).float().mean() cap_acc = (cap2img_match_idx == y).float().mean() return img_acc, cap_acc . Model . If you haven&#39;t used pytorch lightning before, the benefit is that you do not need to stress about which device to put it in, remembering to zero the optimizer etc. All of that is taken care of. Just simply specify the training and validation steps, along with the optimizer and you are good to go. . The other benefit that I really like is logging. You just need to write self.log(&quot;name&quot;, metric_to_track) and it will log to tensorboard by default, or any other kind of logger for that matter. . class Model(pl.LightningModule): def __init__(self, lr: float = 1e-3 ) -&gt; None: super().__init__() self.vision_encoder = VisionEncoder(EMBED_DIM) self.caption_encoder = TextEncoder(EMBED_DIM) self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(TEXT_MODEL)) self.lr = lr def common_step(self, batch: Tuple[torch.Tensor, List[str]]) -&gt; torch.Tensor: images, text = batch device = images.device text_dev = {k: v.to(device) for k, v in self.tokenizer(text).items()} image_embed = self.vision_encoder(images) caption_embed = self.caption_encoder(text_dev) similarity = caption_embed @ image_embed.T loss = clip_loss(similarity) img_acc, cap_acc = metrics(similarity) return loss, img_acc, cap_acc def training_step( self, batch: Tuple[torch.Tensor, List[str]], *args: list ) -&gt; torch.Tensor: loss, img_acc, cap_acc = self.common_step(batch) self.log(&quot;training_loss&quot;, loss, on_step=True) self.log(&quot;training_img_acc&quot;, img_acc, on_step=True, prog_bar=True) self.log(&quot;training_cap_acc&quot;, cap_acc, on_step=True, prog_bar=True) return loss def validation_step( self, batch: Tuple[torch.Tensor, List[str]], *args: list ) -&gt; torch.Tensor: loss, img_acc, cap_acc = self.common_step(batch) self.log(&quot;validation_loss&quot;, loss, on_step=True) self.log(&quot;validation_img_acc&quot;, img_acc, on_step=True, prog_bar=True) self.log(&quot;validation_cap_acc&quot;, cap_acc, on_step=True, prog_bar=True) return loss def configure_optimizers(self) -&gt; torch.optim.Optimizer: vision_params = {&quot;params&quot;: self.vision_encoder.projection.parameters(), &quot;lr&quot;: self.lr} caption_params = {&quot;params&quot;: self.caption_encoder.projection.parameters() , &quot;lr&quot;: self.lr} return torch.optim.Adam([vision_params, caption_params]) . Train . Training is straight forward as show in the five lines below. Using 16 bit precision almost halved the training time from 16 minutes to 9 minutes per epoch. Notice how easy it was to add half precision training and gradient clipping. . model = Model(1e-3) trainer = pl.Trainer( max_epochs= EPOCHS, gpus=torch.cuda.device_count(), gradient_clip_val=1.0, precision=16 ) trainer.fit(model, train_dl, valid_dl) # . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . GPU available: True, used: True . . TPU available: None, using: 0 TPU cores Using native 16bit precision. | Name | Type | Params -- 0 | vision_encoder | VisionEncoder | 21.8 M 1 | caption_encoder | TextEncoder | 135 M -- 1.2 M Trainable params 156 M Non-trainable params 157 M Total params 628.802 Total estimated model params size (MB) . . 1 . Run the following cell if you wish to see the logs in tensorboard. But here&#39;s a screenshot I took: . %reload_ext tensorboard %tensorboard --logdir ./lightning_logs/ . Results . I will compare the text embeddings of the first batch (in the validation set) to all the images of the validation set by taking the dot product between them. . device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;) vision_encoder = model.vision_encoder vision_encoder.eval() vision_encoder = vision_encoder.to(device) image_embeds = [] with torch.no_grad(): for x, _ in tqdm(valid_dl): image_embeds.append(vision_encoder(x.to(device))) image_embed = torch.cat(image_embeds) . . _, y = next(iter(valid_dl)) tokenizer = model.tokenizer caption_encoder = model.caption_encoder caption_encoder.eval() caption_encoder = caption_encoder.to(device) text_dev = {k: v.to(device) for k, v in tokenizer(y).items()} with torch.no_grad(): caption_embed = caption_encoder(text_dev) . similarity = caption_embed @ image_embed.T val, closest = similarity.topk(5, dim=-1) similarity.shape . torch.Size([64, 16557]) . draw_result(i, similarity_matrix) is a convenience function that takes the i-th caption and the similarity matrix, and plots the five closest images, along with the true image. The similarity between the caption and the image is shown in the title. The caption is printed first. . The histogram show the similarity of the caption to all images as a histogram. . Most of the images below are of humans. . draw_result(2, similarity) . A baseball player in the outfield with his hands up, standing next to a team mascot. . Most of the images are of fruits, and one actually has bananas in it. . draw_result(1, similarity) . A watch and clock repair shop window with clocks on display. . You can see most of the images are of animals, and one of them has their head down in the dirt. . draw_result(10, similarity) . A person on a skateboard on the ground. . Below is the google translted version of one of the above captions into spanish. As you can see we actually get a zebra in the fifth image. . text = &quot;Una cebra de pie con la cabeza gacha y comiendo hierba en el suelo de tierra.&quot; text_dev = {k: v.to(device) for k, v in tokenizer(text).items()} with torch.no_grad(): caption_embed_text = caption_encoder(text_dev) similarity_text = caption_embed_text @ image_embed.T . draw_result_single_query(10, similarity_text) . Skateboarder conducting a trick with bicycles in the background. . Again a translated version, this time to french. More humans this time. . text = &quot;Un ordinateur portable est affiché sur une petite plate-forme en bois.&quot; text_dev = {k: v.to(device) for k, v in tokenizer(text).items()} with torch.no_grad(): caption_embed_text = caption_encoder(text_dev) similarity_text = caption_embed_text @ image_embed.T draw_result_single_query(3, similarity_text) . Laptop computer on a small table on the side of a bed . The russian translation below is doing terrible though, so its clearly not bullet proof. Or perhaps I need to train for a bit longer. . text = &quot;Магазин с разными часами&quot; text_dev = {k: v.to(device) for k, v in tokenizer(text).items()} with torch.no_grad(): caption_embed_text = caption_encoder(text_dev) similarity_text = caption_embed_text @ image_embed.T draw_result_single_query(1, similarity_text) . A room filled with clocks through a window. . And lastly I check a single word version. Notice how the dog does kind of look like a bear. Maybe it&#39;s name is bear? . text = &quot;bear&quot; text_dev = {k: v.to(device) for k, v in tokenizer(text).items()} with torch.no_grad(): caption_embed_text = caption_encoder(text_dev) similarity_text = caption_embed_text @ image_embed.T draw_result_single_query(1, similarity_text) . Large collection of digital and analog clocks on display. . Would love to hear any thoughts and comments on the above. . Shameless Self Promotion . Use this link to get 85% off my Machine Learning + Deep Learning course. .",
            "url": "https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html",
            "relUrl": "/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html",
            "date": " • Mar 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Tensorflow Learning Rate Finder",
            "content": "The following tutorial shows how to implement a learning rate finder from scratch, using Keras callbacks. . But first a quick refresher on how we would do model fitting on a simple network: . Imports and Data . import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras.losses import SparseCategoricalCrossentropy %matplotlib inline . (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() x_train = x_train.reshape(len(x_train), -1) x_test = x_test.reshape(len(x_test), -1) # Rescale the images from [0,255] to the [0.0,1.0] range. x_train, x_test = x_train/255.0, x_test/255.0 . Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 11493376/11490434 [==============================] - 0s 0us/step . Model . model = keras.Sequential() model.add(keras.layers.Input(x_train.shape[-1])) model.add(keras.layers.Dense(100, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) . model.fit(x_train, y_train, batch_size=64, epochs=5) . Epoch 1/5 938/938 [==============================] - 3s 3ms/step - loss: 0.5480 - accuracy: 0.8500 Epoch 2/5 938/938 [==============================] - 3s 3ms/step - loss: 0.1601 - accuracy: 0.9546 Epoch 3/5 938/938 [==============================] - 3s 3ms/step - loss: 0.1106 - accuracy: 0.9681 Epoch 4/5 938/938 [==============================] - 2s 3ms/step - loss: 0.0817 - accuracy: 0.9773 Epoch 5/5 938/938 [==============================] - 2s 3ms/step - loss: 0.0632 - accuracy: 0.9811 . &lt;tensorflow.python.keras.callbacks.History at 0x7f60660ca0f0&gt; . LR Finder . Let me outline the logic behind LR finder before we dive into the code. The basic idea is to vary the learning rate and note down the loss. At a certain point when the learning rate is too high the loss will start increasing again. . Therefore the tasks that we have to do in order are: . Get the minimum and maximum learning rate we are willing to look at. | Initialise buffers to hold the learning rate and losses. | Before we begin this process, get the current model weights so we can restore it later. | Get a batch, and get the loss for that batch, and increase the learning rate. | Repeat the above step until maximum learning rate is reached. | Reset old weights to model. | Plot the model. | The above 7 steps can be seen in the LRFind class below. on_train_begin, on_train_batch_end, on_train_end are simply callback functions provided by the keras API. Hopefully, they are self explanatory. . class LRFind(tf.keras.callbacks.Callback): def __init__(self, min_lr, max_lr, n_rounds): self.min_lr = min_lr self.max_lr = max_lr self.step_up = (max_lr / min_lr) ** (1 / n_rounds) self.lrs = [] self.losses = [] def on_train_begin(self, logs=None): self.weights = self.model.get_weights() self.model.optimizer.lr = self.min_lr def on_train_batch_end(self, batch, logs=None): self.lrs.append(self.model.optimizer.lr.numpy()) self.losses.append(logs[&quot;loss&quot;]) self.model.optimizer.lr = self.model.optimizer.lr * self.step_up if self.model.optimizer.lr &gt; self.max_lr: self.model.stop_training = True def on_train_end(self, logs=None): self.model.set_weights(self.weights) . We want to reset the model since it already learnt something decent above, but feel free to skip the next cell to see if results differ. . model = keras.Sequential() model.add(keras.layers.Input(x_train.shape[-1])) model.add(keras.layers.Dense(100, activation=&quot;relu&quot;)) model.add(keras.layers.Dense(10, activation=&quot;softmax&quot;)) model.compile(loss=&quot;sparse_categorical_crossentropy&quot;, optimizer=&quot;adam&quot;, metrics=[&quot;accuracy&quot;]) . Before we go ahead and run learning rate finder, a few things we should define. . First, we need to use tf.data.Dataset.from_tensor_slices incase there aren&#39;t enough batches per epoch for learning rate to go from min_lr to max_lr. | We use EPOCHS=1 but, this is a repeating dataset forever as seen in line 6 below. It is lr_finder_steps that force this repetition to stop at 400 batches. | Instead of model.fit(x_train, y_train,...), we use model.fit(train_dataset). | When plotting we use the log scale since we increase learning rate multiplicatively. | . EPOCHS = 1 BATCH_SIZE = 64 lr_finder_steps = 400 lr_find = LRFind(1e-6, 1e1, lr_finder_steps) train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) .repeat() .shuffle(2048) .batch(BATCH_SIZE) model.fit( train_dataset, steps_per_epoch=lr_finder_steps, epochs=EPOCHS, callbacks=[lr_find] ) plt.plot(lr_find.lrs, lr_find.losses) plt.xscale(&#39;log&#39;) plt.show() . 400/400 [==============================] - 2s 4ms/step - loss: 1.7651 - accuracy: 0.4492 . So looking at the plot above, the minimum occurs at 0.1, however this is most likely going to be unstable. So a good learning rate to use would be 0.01. . Shameless Self Promotion . I have a Machine Learning (and Deep Learning) course on Udemy. If you use the code DEEPSCHOOL2021 you can get the course for $15 instead of the usual $99. .",
            "url": "https://sachinruk.github.io/blog/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html",
            "relUrl": "/tensorflow/learning%20rate/2021/02/15/Tensorflow-Learning-Rate-Finder.html",
            "date": " • Feb 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Focal Loss for Multi-class Classification",
            "content": "class WeightedFocalLoss(nn.Module): &quot;Non weighted version of Focal Loss&quot; def __init__(self, weights, gamma=1.1): super().__init__() self.weights = weights self.gamma = gamma def forward(self, inputs, targets): inputs = inputs.squeeze() targets = targets.squeeze() BCE_loss = F.cross_entropy(inputs, targets, reduction=&#39;none&#39;) pt = torch.exp(-BCE_loss) F_loss = self.weights[targets]*(1-pt)**self.gamma * BCE_loss return F_loss.mean() .",
            "url": "https://sachinruk.github.io/blog/pytorch/loss%20function/2020/11/28/focal-loss.html",
            "relUrl": "/pytorch/loss%20function/2020/11/28/focal-loss.html",
            "date": " • Nov 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sachinruk.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sachinruk.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sachinruk.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}